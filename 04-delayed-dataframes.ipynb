{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://dask.readthedocs.io/en/latest/_images/dask_horizontal.svg\"\n",
    "     align=\"right\"\n",
    "     width=\"30%\"\n",
    "     alt=\"Dask logo\\\">\n",
    "\n",
    "\n",
    "# Delayed DataFrames\n",
    "\n",
    "In two of the previous notebooks we saw two ways to build parallel computations with Dask\n",
    "\n",
    "1.  Use Dask.delayed to wrap custom code\n",
    "2.  Use Dask.dataframe to handle large dataframes \n",
    "\n",
    "Most non-trivial problems require both.  We often deal with tabular data, but messy situations arise where we need to handle things manually.\n",
    "\n",
    "In this notebook we use Dask.delayed to load some custom data and then convert these delayed values into a Dask dataframe.  This shows us how to use both together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "filenames = sorted(glob.glob(os.path.join('data', 'stocks', '*', '*.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dirname = os.path.join('data', 'messy')\n",
    "if os.path.exists(dirname):\n",
    "    import shutil\n",
    "    shutil.rmtree(dirname)\n",
    "    \n",
    "os.mkdir(dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/stocks/GOOG/2015-01-10.csv', parse_dates=['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "((df.timestamp - df.timestamp.dt.floor('1d')).astype(int)/ 1e9).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert(fn):\n",
    "    data, _, symbol, date = fn.split(os.sep)\n",
    "    date = date.split('.')[0]\n",
    "    df = pd.read_csv(fn, parse_dates=['timestamp'])\n",
    "    df['timestamp'] = ((df.timestamp - df.timestamp.dt.floor('1d')).astype(int)/ 1e9).astype(int)\n",
    "    new_fn = os.path.join(data, 'messy', date, symbol + '.feather')\n",
    "    if not os.path.exists(os.path.dirname(new_fn)):\n",
    "        os.mkdir(os.path.dirname(new_fn))\n",
    "    feather.write_dataframe(df, new_fn)\n",
    "\n",
    "import dask\n",
    "import dask.multiprocessing\n",
    "values = [dask.delayed(convert)(fn) for fn in filenames]\n",
    "\n",
    "dask.compute(values, get=dask.multiprocessing.get);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inpsect data\n",
    "\n",
    "```\n",
    "data/messy/2015-01-01\n",
    "├── AAPL.feather\n",
    "├── GOOG.feather\n",
    "├── MSFT.feather\n",
    "└── YHOO.feather\n",
    "data/messy/2015-01-02\n",
    "├── AAPL.feather\n",
    "├── GOOG.feather\n",
    "├── MSFT.feather\n",
    "└── YHOO.feather\n",
    "data/messy/2015-01-03\n",
    "├── AAPL.feather\n",
    "├── GOOG.feather\n",
    "├── MSFT.feather\n",
    "└── YHOO.feather\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import feather\n",
    "df = feather.read_dataframe(os.path.join('data', 'messy', '2015-01-01', 'GOOG.feather'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sequentially, concat to pandas dataframe\n",
    "\n",
    "In the code below we:\n",
    "\n",
    "1.  Load each dataframe into memory\n",
    "2.  Alter the dataframe to include the symbol name and date in the filename\n",
    "3.  Concatenate them into a large dataframe\n",
    "\n",
    "We will eventually want to parallelize this computation using dask.delayed and dask.dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for dir in sorted(glob.glob(os.path.join('data', 'messy', '*'))):\n",
    "    for fn in sorted(glob.glob(os.path.join(dir, '*'))):\n",
    "        _, _, date, symbol = fn.split(os.path.sep)\n",
    "        symbol = symbol[:-len('.feather')]\n",
    "        date = pd.Timestamp(date)\n",
    "        df = feather.read_dataframe(fn)\n",
    "        df['timestamp'] = df.timestamp.astype('m8[s]') + date\n",
    "        df['symbol'] = symbol\n",
    "        dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.concat(dfs, axis=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use dask.dataframe.from_delayed\n",
    "\n",
    "We can construct a Dask.dataframe from many delayed functions that produce pandas dataframes.  Each delayed value forms one of the partitions of the final Dataframe.\n",
    "\n",
    "Consider the following example ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f(n):\n",
    "    return pd.DataFrame({'x': [i for i in range(n)],\n",
    "                         'y': [i ** 2 for i in range(n)]})\n",
    "\n",
    "f(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lazy_dataframes = [dask.delayed(f)(n) for n in [1, 3, 5, 7]]\n",
    "lazy_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "df = dd.from_delayed(lazy_dataframes)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Delayed + Dataframes\n",
    "\n",
    "Build a lazy Dask dataframe from the sequential dataframe munging code we had above.  You will have to use dask.delayed to parallelize/lazify the for-loop code from before and then use `dd.from_delayed` to convert these many lazy Pandas dataframes into a dask.dataframe.\n",
    "\n",
    "*Hint: You may at some point need to rely on [pandas.DataFrame.assign](http://pandas.pydata.org/pandas-docs/version/0.18.1/generated/pandas.DataFrame.assign.html) to avoid mutating a delayed object*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert this code using dask.delayed\n",
    "\n",
    "dfs = []\n",
    "for dir in sorted(glob.glob(os.path.join('data', 'messy', '*'))):\n",
    "    for fn in sorted(glob.glob(os.path.join(dir, '*'))):\n",
    "        _, _, date, symbol = fn.split(os.path.sep)\n",
    "        symbol = symbol[:-len('.feather')]\n",
    "        date = pd.Timestamp(date)\n",
    "        df = feather.read_dataframe(fn)\n",
    "        df['timestamp'] = df.timestamp.astype('m8[s]') + date\n",
    "        df['symbol'] = symbol\n",
    "        dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert delayed values to dask.dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load solutions/04-delayed-dataframes.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
